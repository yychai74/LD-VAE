[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	this thesis comes within the scope of algebraic combinatorics and studies problems related to three orders on permutations the two said weak orders \( right and left \) and the strong order or bruhat order the first part deals with bases of multivariate polynomials most specifically , we study a product of grothendieck polynomials and prove that it can interpreted as a sum over the bruhat order we also present our implementation of grothendieck polynomials and other bases in sage in a second part , we study the tamari order binary trees we obtain a new enumeration formula on the tamari lattice and a new combinatorial prove of chapoton 's functional equation of the generating functions of tamari intervals we extend our results to the m tamari case and thus retrieve a formula given by bousquet m 'elou , pr 'eville ratelle and fusy
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	convolutional neural networks have been shown to develop internal representations , which correspond closely to semantically meaningful objects and parts , although trained solely on class labels class activation mapping \( cam \) is a recent method that makes it possible to easily highlight the image regions contributing to a network 's classification decision we build upon these two developments to enable a network to re examine informative image regions , which we term introspection we propose a weakly supervised iterative scheme , which shifts its center of attention to increasingly discriminative regions as it progresses , by alternating stages of classification and introspection we evaluate our method and show its effectiveness over a range of several datasets , obtaining a top 1 accuracy 84 48 cub 200 2011 , which is the highest to date without using external data or stronger supervision on stanford 40 actions , we set a new state of the art of 87 89 , and on fgvc aircraft and the stanford dogs dataset , we show consistent improvements over baselines , some of which include significantly more supervision
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	let f in mathbb c x , y , s , t be an irreducible constant degree polynomial , and let a , b , c , d subset mathbb c be finite sets of size n we show that f vanishes on at most o \( n 8 3 \) points of the cartesian product a times b times c times d , unless f has a special group related form a similar statement holds for a , b , c , d of unequal sizes this is a four dimensional extension of our recent improved analysis of the original elekes szab 'o theorem in three dimensions
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	motivated by the growing importance of demand response in modern power system 's operations , we propose an architecture and supporting algorithms for privacy preserving thermal inertial load management as a service provided by the load serving entity \( lse \) we focus on an lse managing a population of its customers' air conditioners , and propose a contractual model where the lse guarantees quality of service to each customer in terms of keeping their indoor temperature trajectories within respective bands around the desired individual comfort temperatures we show how the lse can price the contracts differentiated by the flexibility embodied by the width of the specified bands we address architectural questions of \( i \) how the lse can strategize its energy procurement based on price and ambient temperature forecasts , \( ii \) how an lse can close the real time control loop at the aggregate level while providing individual comfort guarantees to loads , without ever measuring the states of an air conditioner for privacy reasons control algorithms to enable our proposed architecture are given , and their efficacy is demonstrated on real data
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	a model of cloud services is emerging whereby a few trusted providers manage the underlying hardware and communications whereas many companies build on this infrastructure to offer higher level , cloud hosted paas services and or saas applications from the start , strong isolation between cloud tenants was seen to be of paramount importance , provided first by virtual machines \( vm \) and later by containers , which share the operating system \( os \) kernel increasingly it is the case that applications also require facilities to effect isolation and protection of data managed by those applications they also require flexible data sharing with other applications , often across the traditional cloud isolation boundaries for example , when government provides many related services for its citizens on a common platform similar considerations apply to the end users of applications but in particular , the incorporation of cloud services within `internet of things' architectures is driving the requirements for both protection and cross application data sharing these concerns relate to the management of data traditional access control is application and principal role specific , applied at policy enforcement points , after which there is no subsequent control over where data flows a crucial issue once data has left its owner 's control by cloud hosted applications and within cloud services information flow control \( ifc \) , in addition , offers system wide , end to end , flow control based on the properties of the data we discuss the potential of cloud deployed ifc for enforcing owners' dataflow policy with regard to protection and sharing , as well as safeguarding against malicious or buggy software in addition , the audit log associated with ifc provides transparency , giving configurable system wide visibility over data flows
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	provided n points in an \( n 1 \) dimensional affine space , and one ordering of the points for each coordinate , we address the problem of testing whether these orderings determine if the points are the vertices of a simplex \( i e are affinely independent \) , regardless of the real values of the coordinates we also attempt to determine the orientation of this simplex in other words , given a matrix whose columns correspond to affine points , we want to know when the sign \( or the non nullity \) of its determinant is implied by orderings given to each row for the values of the row we completely solve the problem in dimensions 2 and 3 we provide a direct combinatorial characterization , along with a formal calculus method it can also be viewed as a decision algorithm , and is based on testing the existence of a suitable inductive cofactor expansion of the determinant we conjecture that our method generalizes in higher dimensions this work aims to be part of a study on how oriented matroids encode shapes of 3 dimensional landmark based objects specifically , applications include the analysis of anatomical data for physical anthropology and clinical research
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	the main approach of traditional information retrieval \( ir \) is to examine how many words from a query appear in a document a drawback of this approach , however , is that it may fail to detect relevant documents where no or only few words from a query are found the semantic analysis methods such as lsa \( latent semantic analysis \) and lda \( latent dirichlet allocation \) have been proposed to address the issue , but their performance is not superior compared to common ir approaches here we present a query document similarity measure motivated by the word mover 's distance unlike other similarity measures , the proposed method relies on neural word embeddings to calculate the distance between words our method is efficient and straightforward to implement the experimental results on trec and pubmed show that our approach provides significantly better performance than bm25 we also discuss the pros and cons of our approach and show that there is a synergy effect when the word embedding measure is combined with the bm25 function
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	we introduce a new quantum adversary method to prove lower bounds on the query complexity of the quantum state generation problem this problem encompasses both , the computation of partial or total functions and the preparation of target quantum states there has been hope for quite some time that quantum state generation might be a route to tackle the sc graph isomorphism problem we show that for the related problem of sc index erasure our method leads to a lower bound of omega \( sqrt n \) which matches an upper bound obtained via reduction to quantum search on n elements this closes an open problem first raised by shi focs'02 our approach is based on two ideas \( i \) on the one hand we generalize the known additive and multiplicative adversary methods to the case of quantum state generation , \( ii \) on the other hand we show how the symmetries of the underlying problem can be leveraged for the design of optimal adversary matrices and dramatically simplify the computation of adversary bounds taken together , these two ideas give the new result for sc index erasure by using the representation theory of the symmetric group also , the method can lead to lower bounds even for small success probability , contrary to the standard adversary method furthermore , we answer an open question due to v s palek ccc'08 by showing that the multiplicative version of the adversary method is stronger than the additive one for any problem finally , we prove that the multiplicative bound satisfies a strong direct product theorem , extending a result by v s palek to quantum state generation problems
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	in this paper we develop novel results on self triggering control of nonlinear systems , subject to perturbations and actuation delays first , considering an unperturbed nonlinear system with bounded actuation delays , we provide conditions that guarantee the existence of a self triggering control strategy stabilizing the closed loop system then , considering parameter uncertainties , disturbances , and bounded actuation delays , we provide conditions guaranteeing the existence of a self triggering strategy , that keeps the state arbitrarily close to the equilibrium point in both cases , we provide a methodology for the computation of the next execution time we show on an example the relevant benefits obtained with this approach , in terms of energy consumption , with respect to control algorithms based on a constant sampling , with a sensible reduction of the average sampling time
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	nowadays , scientific challenges usually require approaches that cross traditional boundaries between academic disciplines , driving many researchers towards interdisciplinarity despite its obvious importance , there is a lack of studies on how to quantify the influence of interdisciplinarity on the research impact , posing uncertainty in a proper evaluation for hiring and funding purposes here we propose a method based on the analysis of bipartite interconnected multilayer networks of citations and disciplines , to assess scholars , institutions and countries interdisciplinary importance using data about physics publications and us patents , we show that our method allows to reveal , using a quantitative approach , that being more interdisciplinary causes in the granger sense benefits in scientific productivity and impact the proposed method could be used by funding agencies , universities and scientific policy decision makers for hiring and funding purposes , and to complement existing methods to rank universities and countries
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	an edge coloring of a graph g with colors 1 , 2 , ldots , t is an interval t coloring if all colors are used , and the colors of edges incident to each vertex of g are distinct and form an interval of integers a graph g is interval colorable if it has an interval t coloring for some positive integer t for an interval colorable graph g , w \( g \) denotes the greatest value of t for which g has an interval t coloring it is known that the complete graph is interval colorable if and only if the number of its vertices is even however , the exact value of w \( k 2n \) is known only for n leq 4 the second author showed that if n p2 q , where p is odd and q is nonnegative , then w \( k 2n \) geq 4n 2 p q later , he conjectured that if n in mathbb n , then w \( k 2n \) 4n 2 left lfloor log 2 n right rfloor left n 2 right , where left n 2 right is the number of 1 's in the binary representation of n in this paper we introduce a new technique to construct interval colorings of complete graphs based on their 1 factorizations , which is used to disprove the conjecture , improve lower and upper bounds on w \( k 2n \) and determine its exact values for n leq 12
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	we prove a version of the quantum de finetti theorem permutation invariant quantum states are well approximated as a probabilistic mixture of independent and identically distributed states the approximation is measured by distinguishability under one way locc \( local operations and classical communication \) measurements indeed , we consider approximation with respect to the fully one way locc norm or relative entropy , compared to the parallel one way locc norm of earlier work , while the error bound is kept essentially the same we apply our new de finetti theorem to the problems considered in brandao and harrow , arxiv 1210 6367 , and obtain several new or improved results of particular interest are a quasipolynomial time algorithm which distinguishes multipartite density matrices with entanglement larger than a small constant amount \( measured with a variant of the relative entropy of entanglement \) from separable , and a proof that in quantum merlin arthur proof systems , polynomially many provers are not more powerful than a single prover when the verifier is restricted to one way locc operations
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	in this paper , we consider the problem of minimizing a linear functional subject to uncertain linear and bilinear matrix inequalities , which depend in a possibly nonlinear way on a vector of uncertain parameters motivated by recent results in statistical learning theory , we show that probabilistic guaranteed solutions can be obtained by means of randomized algorithms in particular , we show that the vapnik chervonenkis dimension \( vc dimension \) of the two problems is finite , and we compute upper bounds on it in turn , these bounds allow us to derive explicitly the sample complexity of these problems using these bounds , in the second part of the paper , we derive a sequential scheme , based on a sequence of optimization and validation steps the algorithm is on the same lines of recent schemes proposed for similar problems , but improves both in terms of complexity and generality the effectiveness of this approach is shown using a linear model of a robot manipulator subject to uncertain parameters
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	a fundamental result of mader from 1972 asserts that a graph of high average degree contains a highly connected subgraph with roughly the same average degree we prove a lemma showing that one can strengthen mader 's result by replacing the notion of high connectivity by the notion of vertex expansion another well known result in graph theory states that for every integer t there is a smallest real c \( t \) so that every n vertex graph with c \( t \) n edges contains a k t minor fiorini , joret , theis and wood conjectured that if an n vertex graph g has \( c \( t \) epsilon \) n edges then g contains a k t minor of order at most c \( epsilon \) log n we use our extension of mader 's theorem to prove that such a graph g must contain a k t minor of order at most c \( epsilon \) log n loglog n known constructions of graphs with high girth show that this result is tight up to the loglog n factor
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	in order to effectively prepare the next generation of it professionals and systems analysts , it is important to incorporate cloud based online collaboration tools into the coursework for developing the students' cooperative skills as well as for storing and sharing content for these pedagogical and practical reasons , google drive has been used at a medium sized institution of higher education in new zealand during the systems analysis and design course ongoing and successful use of any learning technology requires gathering meaningful feedback from students , and acting as a mentor during their learning journey this study has been developed and implemented to help students enjoy the collaborative technology and to help increase their satisfaction and commitment in order to overcome the obstacles that may prevent students from using google drive optimally , an initial survey has been conducted to better understand the influential factors and issues furthermore , this study aims at promoting various types of collaboration and sharing seeing and learning from other students' work , receiving direct suggestions from others , and allowing others to edit documents that belong to them following the results of the first quantitative survey , numerous teaching strategies were formulated and implemented a final qualitative survey was done at the end of the course for students to evaluate their project work the results of this study also provide original practical and theoretical implications that may be of interest to other researchers , course designers , and teachers
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	one of the challenges in modeling cognitive events from electroencephalogram \( eeg \) data is finding representations that are invariant to inter and intra subject differences , as well as to inherent noise associated with such data herein , we propose a novel approach for learning such representations from multi channel eeg time series , and demonstrate its advantages in the context of mental load classification task first , we transform eeg activities into a sequence of topology preserving multi spectral images , as opposed to standard eeg analysis techniques that ignore such spatial information next , we train a deep recurrent convolutional network inspired by state of the art video classification to learn robust representations from the sequence of images the proposed approach is designed to preserve the spatial , spectral , and temporal structure of eeg which leads to finding features that are less sensitive to variations and distortions within each dimension empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state of the art approaches in this field
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	this paper develops an algorithm that identifies and decomposes a median graph of a triangulation of a 2 dimensional \( 2d \) oriented bordered surface and in addition restores all corresponding triangulation whenever they exist the algorithm is based on the consecutive simplification of the given graph by reducing degrees of its nodes from the paper cite fst1 , it is known that such graphs can not have nodes of degrees above 8 neighborhood of nodes of degrees 8 , 7 , 6 , 5 , and 4 are consecutively simplified then , a criterion is provided to identify median graphs with nodes of degrees at most 3 as a byproduct , we produce an algorithm that is more effective than previous known to determine quivers of finite mutation type of size greater than 10
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	unsupervised learning and supervised learning are key research topics in deep learning however , as high capacity supervised neural networks trained with a large amount of labels have achieved remarkable success in many computer vision tasks , the availability of large scale labeled images reduced the significance of unsupervised learning inspired by the recent trend toward revisiting the importance of unsupervised learning , we investigate joint supervised and unsupervised learning in a large scale setting by augmenting existing neural networks with decoding pathways for reconstruction first , we demonstrate that the intermediate activations of pretrained large scale classification networks preserve almost all the information of input images except a portion of local spatial details then , by end to end training of the entire augmented architecture with the reconstructive objective , we show improvement of the network performance for supervised tasks we evaluate several variants of autoencoders , including the recently proposed what where autoencoder that uses the encoder pooling switches , to study the importance of the architecture design taking the 16 layer vggnet trained under the imagenet ilsvrc 2012 protocol as a strong baseline for image classification , our methods improve the validation set accuracy by a noticeable margin
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	von neumann 's min max theorem guarantees that each player of a zero sum matrix game has an optimal mixed strategy this paper gives an elementary proof that each player has a near optimal mixed strategy that chooses uniformly at random from a multiset of pure strategies of size logarithmic in the number of pure strategies available to the opponent for exponentially large games , for which even representing an optimal mixed strategy can require exponential space , it follows that there are near optimal , linear size strategies these strategies are easy to play and serve as small witnesses to the approximate value of the game as a corollary , it follows that every language has small ``hard'' multisets of inputs certifying that small circuits ca n't decide the language for example , if sat does not have polynomial size circuits , then , for each n and c , there is a set of n \( o \( c \) \) boolean formulae of size n such that no circuit of size n c \( or algorithm running in time n c \) classifies more than two thirds of the formulae succesfully
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	clustering is an unsupervised learning method that constitutes a cornerstone of an intelligent data analysis process it is used for the exploration of inter relationships among a collection of patterns , by organizing them into homogeneous clusters clustering has been dynamically applied to a variety of tasks in the field of information retrieval \( ir \) clustering has become one of the most active area of research and the development clustering attempts to discover the set of consequential groups where those within each group are more closely related to one another than the others assigned to different groups the resultant clusters can provide a structure for organizing large bodies of text for efficient browsing and searching there exists a wide variety of clustering algorithms that has been intensively studied in the clustering problem among the algorithms that remain the most common and effectual , the iterative optimization clustering algorithms have been demonstrated reasonable performance for clustering , e g the expectation maximization \( em \) algorithm and its variants , and the well known k means algorithm this paper presents an analysis on how partition method clustering techniques em , k means and k means algorithm work on heartspect dataset with below mentioned features purity , entropy , cpu time , cluster wise analysis , mean value analysis and inter cluster distance thus the paper finally provides the experimental results of datasets for five clusters to strengthen the results that the quality of the behavior in clusters in em algorithm is far better than k means algorithm and k means algorithm
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	in this work we show that deep convolutional neural networks can outperform humans on the task of boundary detection , as measured on the standard berkeley segmentation dataset our detector is fully integrated in the popular caffe framework and processes a 320x420 image in less than a second our contributions consist firstly in combining a careful design of the loss for boundary detection training , a multi resolution architecture and training with external data to improve the detection accuracy of the current state of the art , from an optimal dataset scale f measure of 0 780 to 0 808 while human performance is at 0 803 we further improve performance to 0 813 by combining deep learning with grouping , integrating the normalized cuts technique within a deep network we also examine the potential of our boundary detector in conjunction with the higher level tasks of object proposal generation and semantic segmentation for both tasks our detector yields clear improvements over state of the art systems
[1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	an important property of high performance , low complexity codes is the existence of highly efficient algorithms for their decoding many of the most efficient , recent graph based algorithms , e g message passing algorithms and decoding based on linear programming , crucially depend on the efficient representation of a code in a graphical model in order to understand the performance of these algorithms , we argue for the characterization of codes in terms of a so called fundamental cone in euclidean space which is a function of a given parity check matrix of a code , rather than of the code itself we give a number of properties of this fundamental cone derived from its connection to unramified covers of the graphical models on which the decoding algorithms operate for the class of cycle codes , these developments naturally lead to a characterization of the fundamental polytope as the newton polytope of the hashimoto edge zeta function of the underlying graph
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	this paper contributes to the challenging field of security for wireless sensor networks by introducing a key agreement scheme in which sensor nodes create secure radio connections with their neighbours depending on the aid of third parties these third parties are responsible only for the pair wise key establishment among sensor nodes , so they do not observe the physical phenomenon nor route data packets to other nodes the proposed method is explained here with respect to four important issues how secret shares are distributed , how local neighbours are discovered , how legitimate third parties are verified , and how secure channels are established moreover , the performance of the scheme is analyzed with regards to five metrics local connectivity , resistance to node capture , memory usage , communication overhead , and computational burden our scheme not only secures the transmission channels of nodes but also guarantees high local connectivity of the sensor network , low usage of memory resources , perfect network resilience against node capture , and complete prevention against impersonation attacks as it is demonstrated in this paper , using a number of third parties equals to 10 of the total number of sensor nodes in the area of interest , the proposed method can achieve at least 99 42 local connectivity with a very low usage of available storage resources \( less than 385 bits on each sensor node \)
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	let g be a simple graph with vertex set v \( g \) a set s is independent if no two vertices from s are adjacent the graph g is known to be a konig egervary if alpha \( g \) mu \( g \) v \( g \) , where alpha \( g \) denotes the size of a maximum independent set and mu \( g \) is the cardinality of a maximum matching the number d \( x \) x n \( x \) is the difference of x , and an independent set a is critical if d \( a \) max d \( i \) i is an independent set in g \( zhang 1990 \) let omega \( g \) denote the family of all maximum independent sets let us say that a family gamma of independent sets is a konig egervary collection if union of gamma intersection of gamma 2alpha \( g \) \( jarden , levit , mandrescu 2015 \) in this paper , we show that if the family of all maximum critical independent sets of a graph g is a konig egervary collection , then g is a konig egervary graph it generalizes one of our conjectures recently validated in \( short 2015 \)
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	the era of big data has spawned unprecedented interests in developing hashing algorithms for efficient storage and fast nearest neighbor search most existing work learn hash functions that are numeric quantizations of feature values in projected feature space in this work , we propose a novel hash learning framework that encodes feature 's rank orders instead of numeric values in a number of optimal low dimensional ranking subspaces we formulate the ranking subspace learning problem as the optimization of a piece wise linear convex concave function and present two versions of our algorithm one with independent optimization of each hash bit and the other exploiting a sequential learning framework our work is a generalization of the winner take all \( wta \) hash family and naturally enjoys all the numeric stability benefits of rank correlation measures while being optimized to achieve high precision at very short code length we compare with several state of the art hashing algorithms in both supervised and unsupervised domain , showing superior performance in a number of data sets
[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	two ubiquitous aspects of large scale data analysis are that the data often have heavy tailed properties and that diffusion based or spectral based methods are often used to identify and extract structure of interest perhaps surprisingly , popular distribution independent methods such as those based on the vc dimension fail to provide nontrivial results for even simple learning problems such as binary classification in these two settings in this paper , we develop distribution dependent learning methods that can be used to provide dimension independent sample complexity bounds for the binary classification problem in these two popular settings in particular , we provide bounds on the sample complexity of maximum margin classifiers when the magnitude of the entries in the feature vector decays according to a power law and also when learning is performed with the so called diffusion maps kernel both of these results rely on bounding the annealed entropy of gap tolerant classifiers in a hilbert space we provide such a bound , and we demonstrate that our proof technique generalizes to the case when the margin is measured with respect to more general banach space norms the latter result is of potential interest in cases where modeling the relationship between data elements as a dot product in a hilbert space is too restrictive
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	in this paper , we investigate the distributed shortest distance optimization problem for a multi agent network to cooperatively minimize the sum of the quadratic distances from some convex sets , where each set is only associated with one agent to deal with the optimization problem with projection uncertainties , we propose a distributed continuous time dynamical protocol based on a new concept of approximate projection here each agent can only obtain an approximate projection point on the boundary of its convex set , and communicate with its neighbors over a time varying communication graph first , we show that no matter how large the approximate angle is , the system states are always bounded for any initial condition , and uniformly bounded with respect to all initial conditions if the inferior limit of the stepsize is greater than zero then , in the two cases , nonempty intersection and empty intersection of convex sets , we provide stepsize and approximate angle conditions to ensure the optimal convergence , respectively moreover , we give some characterizations about the optimal solutions for the empty intersection case and also present the convergence error between agents' estimates and the optimal point in the case of constant stepsizes and approximate angles
[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	a genetic algorithm \( ga \) is proposed in which each member of the population can change schemata only with its neighbors according to a rule the rule methodology and the neighborhood structure employ elements from the cellular automata \( ca \) strategies each member of the ga population is assigned to a cell and crossover takes place only between adjacent cells , according to the predefined rule although combinations of ca and ga approaches have appeared previously , here we rely on the inherent self organizing features of ca , rather than on parallelism this conceptual shift directs us toward the evolution of compact populations containing only a handful of members we find that the resulting algorithm can search the design space more efficiently than traditional ga strategies due to its ability to exploit mutations within this compact self organizing population consequently , premature convergence is avoided and the final results often are more accurate in order to reinforce the superior mutation capability , a re initialization strategy also is implemented ten test functions and two benchmark structural engineering truss design problems are examined in order to demonstrate the performance of the method
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	the three in a tree algorithm of chudnovsky and seymour decides in time o \( n 4 \) whether three given vertices of a graph belong to an induced tree here , we study four in a tree for triangle free graphs we give a structural answer to the following question what does a triangle free graph look like if no induced tree covers four given vertices \? our main result says that any such graph must have the same structure , in a sense to be defined precisely , as a square or a cube we provide an o \( nm \) time algorithm that given a triangle free graph g together with four vertices outputs either an induced tree that contains them or a partition of v \( g \) certifying that no such tree exists we prove that the problem of deciding whether there exists a tree t covering the four vertices such that at most one vertex of t has degree at least 3 is np complete
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	let g s be a solvable permutation group of the symmetric group s n given as input by the generating set s we give a deterministic polynomial time algorithm that computes an emph expanding generating set of size tilde o \( n 2 \) for g more precisely , the algorithm computes a subset t subset g of size tilde o \( n 2 \) \( 1 lambda \) o \( 1 \) such that the undirected cayley graph cay \( g , t \) is a lambda spectral expander \( the tilde o notation suppresses log o \( 1 \) n factors \) as a byproduct of our proof , we get a new explicit construction of varepsilon bias spaces of size tilde o \( n poly \( log d \) \) \( frac 1 varepsilon \) o \( 1 \) for the groups z d n the earlier known size bound was o \( \( d n varepsilon 2 \) \) 11 2 given by cite amn98
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	quantum query complexity is known to be characterized by the so called quantum adversary bound while this result has been proved in the standard discrete time model of quantum computation , it also holds for continuous time \( or hamiltonian based \) quantum computation , due to a known equivalence between these two query complexity models in this work , we revisit this result by providing a direct proof in the continuous time model one originality of our proof is that it draws new connections between the adversary bound , a modern theoretical computer science technique , and early theorems of quantum mechanics indeed , the proof of the lower bound is based on ehrenfest 's theorem , while the upper bound relies on the adiabatic theorem , as we construct an optimal adiabatic quantum query algorithm
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	we present a method to design distributed generation and demand control schemes for primary frequency regulation in power networks that guarantee asymptotic stability and ensure fairness of allocation we impose a passivity condition on net power supply variables and provide explicit steady state conditions on a general class of generation and demand control dynamics that ensure convergence of solutions to equilibria that solve an appropriately constructed network optimization problem we also show that the inclusion of controllable demand results in a drop in steady state frequency deviations we discuss how various classes of dynamics used in recent studies fit within our framework and show that this allows for less conservative stability and optimality conditions we illustrate our results with simulations on the ieee 68 bus system and observe that both static and dynamic demand response schemes that fit within our framework offer improved transient and steady state behavior compared with control of generation alone the dynamic scheme is also seen to enhance the robustness of the system to time delays
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	in this article , we investigate the use of a probabilistic model for unsupervised clustering in text collections unsupervised clustering has become a basic module for many intelligent text processing applications , such as information retrieval , text classification or information extraction the model considered in this contribution consists of a mixture of multinomial distributions over the word counts , each component corresponding to a different theme we present and contrast various estimation procedures , which apply both in supervised and unsupervised contexts in supervised learning , this work suggests a criterion for evaluating the posterior odds of new documents which is more statistically sound than the naive bayes approach in an unsupervised context , we propose measures to set up a systematic evaluation framework and start with examining the expectation maximization \( em \) algorithm as the basic tool for inference we discuss the importance of initialization and the influence of other features such as the smoothing strategy or the size of the vocabulary , thereby illustrating the difficulties incurred by the high dimensionality of the parameter space we also propose a heuristic algorithm based on iterative em with vocabulary reduction to solve this problem using the fact that the latent variables can be analytically integrated out , we finally show that gibbs sampling algorithm is tractable and compares favorably to the basic expectation maximization approach
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	in this paper , we introduce a significant extension , called scenario with certificates \( swc \) , of the so called scenario approach for uncertain optimization problems this extension is motivated by the observation that in many control problems only some of the optimization variables are used in the design phase , while the other variables play the role of certificates examples are all those control problems that can be reformulated in terms of linear matrix inequalities involving parameter dependent lyapunov functions these control problems include static anti windup compensator design for uncertain linear systems with input saturation , where the goal is the minimization of the nonlinear gain from an exogenous input to a performance output the main contribution of this paper is to show that randomization is a useful tool , specifically for anti windup design , to make the overall approach less conservative compared to its robust counterpart in particular , we demonstrate that the scenario with certificates reformulation is appealing because it provides a way to implicitly design the parameter dependent lyapunov functions finally , to further reduce the computational cost of this one shot approach , we present a sequential randomized algorithm for iteratively solving this problem
[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	over the last few years , more and more heuristic decision making techniques have been inspired by nature , e g evolutionary algorithms , ant colony optimisation and simulated annealing more recently , a novel computational intelligence technique inspired by immunology has emerged , called artificial immune systems \( ais \) this immune system inspired technique has already been useful in solving some computational problems in this keynote , we will very briefly describe the immune system metaphors that are relevant to ais we will then give some illustrative real world problems suitable for ais use and show a step by step algorithm walkthrough a comparison of ais to other well known algorithms and areas for future work will round this keynote off it should be noted that as ais is still a young and evolving field , there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from the examples given here
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	the number of citations received by authors in scientific journals has become a major parameter to assess individual researchers and the journals themselves through the impact factor a fair assessment therefore requires that the criteria for selecting references in a given manuscript should be unbiased with respect to the authors or the journals cited in this paper , we advocate that authors should follow two mandatory principles to select papers \( later reflected in the list of references \) while studying the literature for a given research i \) consider similarity of content with the topics investigated , lest very related work should be reproduced or ignored ii \) perform a systematic search over the network of citations including seminal or very related papers we use formalisms of complex networks for two datasets of papers from the arxiv repository to show that neither of these two criteria is fulfilled in practice
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	we show that m points and n smooth algebraic surfaces of bounded degree in mathbb r 3 satisfying suitable nondegeneracy conditions can have at most o \( m frac 2k 3k 1 n frac 3k 3 3k 1 m n \) incidences , provided that any collection of k points have at most o \( 1 \) surfaces passing through all of them , for some k geq 3 in the case where the surfaces are spheres and no three spheres meet in a common circle , this implies there are o \( \( mn \) 3 4 m n \) point sphere incidences this is a slight improvement over the previous bound of o \( \( mn \) 3 4 beta \( m , n \) m n \) for beta \( m , n \) an \( explicit \) very slowly growing function we obtain this bound by using the discrete polynomial ham sandwich theorem to cut mathbb r 3 into open cells adapted to the set of points , and within each cell of the decomposition we apply a turan type theorem to obtain crude control on the number of point surface incidences we then perform a second polynomial ham sandwich decomposition on the irreducible components of the variety defined by the first decomposition as an application , we obtain a new bound on the maximum number of unit distances amongst m points in mathbb r 3
[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	on line linear optimization on combinatorial action sets \( d dimensional actions \) with bandit feedback , is known to have complexity in the order of the dimension of the problem the exponential weighted strategy achieves the best known regret bound that is of the order of d 2 sqrt n \( where d is the dimension of the problem , n is the time horizon \) however , such strategies are provably suboptimal or computationally inefficient the complexity is attributed to the combinatorial structure of the action set and the dearth of efficient exploration strategies of the set mirror descent with entropic regularization function comes close to solving this problem by enforcing a meticulous projection of weights with an inherent boundary condition entropic regularization in mirror descent is the only known way of achieving a logarithmic dependence on the dimension here , we argue otherwise and recover the original intuition of exponential weighting by borrowing a technique from discrete optimization and approximation algorithms called `extended formulation' such formulations appeal to the underlying geometry of the set with a guaranteed logarithmic dependence on the dimension underpinned by an information theoretic entropic analysis
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	this article is concerned with an example of complex planar geometry arising from flat origami challenges the complexity of solution algorithms is illustrated , depending on the depth of the initial analysis of the problem , starting from brute force enumeration , up to the equivalence to a dedicated problem in graph theory this leads to algorithms starting from an untractable case on modern computers , up to a run of few seconds on a portable personal computer this emphasizes the need for a prior analysis by humans before considering the assistance of computers for complex design problems the graph problem is an enumeration of spanning trees from a grid graph , leading to a coarse scale description of the topology of the paper edge on the flat folded state
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	rating and recommendation systems have become a popular application area for applying a suite of machine learning techniques current approaches rely primarily on probabilistic interpretations and extensions of matrix factorization , which factorizes a user item ratings matrix into latent user and item vectors most of these methods fail to model significant variations in item ratings from otherwise similar users , a phenomenon known as the napoleon dynamite effect recent efforts have addressed this problem by adding a contextual bias term to the rating , which captures the mood under which a user rates an item or the context in which an item is rated by a user in this work , we extend this model in a nonparametric sense by learning the optimal number of moods or contexts from the data , and derive gibbs sampling inference procedures for our model we evaluate our approach on the movielens 1m dataset , and show significant improvements over the optimal parametric baseline , more than twice the improvements previously encountered for this task we also extract and evaluate a dblp dataset , wherein we predict the number of papers co authored by two authors , and present improvements over the parametric baseline on this alternative domain as well
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	citation metrics are becoming pervasive in the quantitative evaluation of scholars , journals and institutions more then ever before , hiring , promotion , and funding decisions rely on a variety of impact metrics that cannot disentangle quality from productivity , and are biased by factors such as discipline and academic age biases affecting the evaluation of single papers are compounded when one aggregates citation based metrics across an entire publication record it is not trivial to compare the quality of two scholars that during their careers have published at different rates in different disciplines in different periods of time we propose a novel solution based on the generation of a statistical baseline specifically tailored on the academic profile of each researcher by decoupling productivity and impact , our method can determine whether a certain level of impact can be explained by productivity alone , or additional ingredients of scientific excellence are necessary the method is flexible enough to allow for the evaluation of , and fair comparison among , arbitrary collections of papers scholar publication records , journals , and entire institutions and can be extended to simultaneously suppresses any source of bias we show that our method can capture the quality of the work of nobel laureates irrespective of productivity , academic age , and discipline , even when traditional metrics indicate low impact in absolute terms we further apply our methodology to almost a million scholars and over six thousand journals to quantify the impact required to demonstrate scientific excellence for a given level of productivity
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	predicting user responses , such as click through rate and conversion rate , are critical in many web applications including web search , personalised recommendation , and online advertising different from continuous raw features that we usually found in the image and audio domains , the input features in web space are always of multi field and are mostly discrete and categorical while their dependencies are little known major user response prediction models have to either limit themselves to linear models or require manually building up high order combination features the former loses the ability of exploring feature interactions , while the latter results in a heavy computation in the large feature space to tackle the issue , we propose two novel models using deep neural networks \( dnns \) to automatically learn effective patterns from categorical feature interactions and make predictions of users' ad clicks to get our dnns efficiently work , we propose to leverage three feature transformation methods , i e , factorisation machines \( fms \) , restricted boltzmann machines \( rbms \) and denoising auto encoders \( daes \) this paper presents the structure of our models and their efficient training algorithms the large scale experiments with real world data demonstrate that our methods work better than major state of the art models
[1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	we provide a unified treatment of a broad class of noisy structure recovery problems , known as structured normal means problems in these problems , the goal is to identify , from a finite collection of gaussian distributions with different means , the distribution that produced the observed data recent work has studied several special cases including sparse vectors , biclusters , and graph based structures we establish nearly matching upper and lower bounds on the minimax probability of error for any structured normal means problem we also derive an optimality certificate for the maximum likelihood estimator , which can be applied to many instantiations we consider an experimental design setting , where we generalize our minimax bounds and derive an algorithm for computing a design strategy with a certain optimality property we show that our results give tight minimax bounds for many structure recovery problems and consider some consequences for interactive sampling
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	this paper addresses the design of input signals for the purpose of discriminating among a finite set of models dynamic systems within a given finite time interval a motivating application is fault detection and isolation we propose several specific optimization problems , with objectives or constraints based on signal power , signal amplitude , and probability of successful model discrimination since these optimization problems are nonconvex , we suggest a suboptimal solution via a random search algorithm guided by the semidefinite relaxation \( sdr \) and analyze the accuracy of the suboptimal solution we conclude with a simple example taken from a benchmark problem on fault detection for wind turbines
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	opinion mining and sentiment analysis have emerged as a field of study since the widespread of world wide web and internet opinion refers to extraction of those lines or phrase in the raw and huge data which express an opinion sentiment analysis on the other hand identifies the polarity of the opinion being extracted in this paper we propose the sentiment analysis in collaboration with opinion extraction , summarization , and tracking the records of the students the paper modifies the existing algorithm in order to obtain the collaborated opinion about the students the resultant opinion is represented as very high , high , moderate , low and very low the paper is based on a case study where teachers give their remarks about the students and by applying the proposed sentiment analysis algorithm the opinion is extracted and represented
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	a graph g is said to be 1 perfectly orientable \( 1 p o for short \) if it admits an orientation such that the out neighborhood of every vertex is a clique in g the class of 1 p o graphs forms a common generalization of the classes of chordal and circular arc graphs even though 1 p o graphs can be recognized in polynomial time , no structural characterization of 1 p o graphs is known in this paper we consider the four standard graph products the cartesian product , the strong product , the direct product , and the lexicographic product for each of them , we characterize when a nontrivial product of two graphs is 1 p o
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	a graph g is said to be a `set graph' if it admits an acyclic orientation that is also `extensional' , in the sense that the out neighborhoods of its vertices are pairwise distinct equivalently , a set graph is the underlying graph of the digraph representation of a hereditarily finite set in this paper , we continue the study of set graphs and related topics , focusing on computational complexity aspects we prove that set graph recognition is np complete , even when the input is restricted to bipartite graphs with exactly two leaves the problem remains np complete if , in addition , we require that the extensional acyclic orientation be also `slim' , that is , that the digraph obtained by removing any arc from it is not extensional we also show that the counting variants of the above problems are p complete , and prove similar complexity results for problems related to a generalization of extensional acyclic digraphs , the so called `hyper extensional digraphs' , which were proposed by aczel to describe hypersets our proofs are based on reductions from variants of the hamiltonian path problem we also consider a variant of the well known notion of a separating code in a digraph , the so called `open out separating code' , and show that it is np complete to determine whether an input extensional acyclic digraph contains an open out separating code of given size
[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	a linear coloring of a graph is a proper coloring of the vertices of the graph so that each pair of color classes induce a union of disjoint paths in this paper , we prove that for every connected graph with maximum degree at most three and every assignment of lists of size four to the vertices of the graph , there exists a linear coloring such that the color of each vertex belongs to the list assigned to that vertex and the neighbors of every degree two vertex receive different colors , unless the graph is c 5 or k 3 , 3 this confirms a conjecture raised by esperet , montassier , and raspaud our proof is constructive and yields a linear time algorithm to find such a coloring
[1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	this paper is concerned with the hard thresholding technique which sets all but the k largest absolute elements to zero we establish a tight bound that quantitatively characterizes the deviation of the thresholded solution from a given signal our theoretical result is universal in the sense that it holds for all choices of parameters , and the underlying analysis only depends on fundamental arguments in mathematical optimization we discuss the implications for the literature compressed sensing on account of the crucial estimate , we bridge the connection between restricted isometry property \( rip \) and the sparsity parameter of k for a vast volume of hard thresholding based algorithms , which renders an improvement on the rip condition especially when the true sparsity is unknown this suggests that in essence , many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure machine learning in terms of large scale machine learning , a significant yet challenging problem is producing sparse solutions in online setting in stark contrast to prior works that attempted the ell 1 relaxation for promoting sparsity , we present a novel algorithm which performs hard thresholding in each iteration to ensure such parsimonious solutions equipped with the developed bound for hard thresholding , we prove global linear convergence for a number of prevalent statistical models under mild assumptions , even though the problem turns out to be non convex
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]	lhc era henp experiments will generate unprecidented volumes of data and require commensurately large compute resources these resources are larger than can be marshalled at any one site within the community production reconstruction , analysis , and simulation will need to take maximum advantage of these distributed computing and storage resources using the new capabilities offered by the grid computing paradigm since large scale , coordinated grid computing involves user access across many regional centers and national and funding boundaries , one of the most crucial aspects of grid computing is that of user authentication and authorization while projects such as the doe grids ca have gone a long way to solving the problem of distributed authentication , the authorization problem is still largely open we have developed and tested a prototype vo role management system using the community authorization service \( cas \) from the globus project cas allows for a flexible definition of resources in this protoype we define a role as a resource within the cas database and assign individuals in the vo access to that resource to indicate their ability to assert the role the access of an individual to this vo role resource is then an annotation of the user 's cas proxy certificate this annotation is then used by the local resource managers to authorize access to local compute and storage resources at a granularity which is base on neither vos nor individuals we report here on the configuration details for the cas database and the globus gatekeeper and on how this general approch could be formalized and extended to meet the clear needs of lhc experiments using the grid
